{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended as a companion to the paper [Robust inference of neutron-star parameters from thermonuclear burst observations (Galloway et al. 2022, ApJS 263 30)](https://iopscience.iop.org/article/10.3847/1538-4365/ac98c9), and includes simple examples of using methods as part of the [`concord` repository](https://github.com/outs1der/concord) to demonstrate the inference of neutron star properties & burst fuel composition from observations and comparison with simulated data. \n",
    "\n",
    "The sections below largely follow the discussion in the paper, and include examples of data analysis from \n",
    "1. 4U 0513+40\n",
    "1. Kepler simulations\n",
    "1. GS 1826-238\n",
    "1. XMMU J181227.8-181234 (= XTE J1812-182)\n",
    "1. IGR J17591-2342\n",
    "1. IGR J00291+5934\n",
    "\n",
    "The analysis approaches are broken into cases depending on the extent of the available data; e.g. pairs (or trains) of bursts with well-constrained recurrence times, all the way down to no bursts at all. See below for these examples\n",
    "\n",
    "The notebook has been tested with the following packages:\n",
    "* `python` 3.9.10\n",
    "* `matplotlib` 3.5.2\n",
    "* `astropy` 5.1\n",
    "* `ChainConsumer` 0.33.0\n",
    "* `astroquery` 0.4.5\n",
    "* `scipy` 1.7.3\n",
    "\n",
    "See `README.md` for additional instructions on getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concord as cd\n",
    "print ('concord version {}'.format(cd.__version__))\n",
    "\n",
    "import sys\n",
    "import astropy\n",
    "from astropy.io import ascii\n",
    "from astropy.table import Table\n",
    "import astropy.units as u\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import astropy.constants as const\n",
    "from astropy.visualization import quantity_support \n",
    "import astropy.uncertainty as unc\n",
    "\n",
    "from chainconsumer import ChainConsumer\n",
    "from numpy.random import normal, multivariate_normal\n",
    "import math\n",
    "\n",
    "flux_unit = u.erg/u.cm**2/u.s\n",
    "mdot_Edd = 8.8e4*u.g/u.cm**2/u.s\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Check all the versions here\n",
    "\n",
    "print ('python {}'.format(sys.version.split(' ')[0]))\n",
    "print ('matplotlib {}'.format(matplotlib.__version__))\n",
    "print ('astropy {}'.format(astropy.__version__))\n",
    "print ('ChainConsumer {}'.format(ChainConsumer.__version__))\n",
    "# print ('astroquery {}'.format(astroquery.__version__))\n",
    "# print ('scipy {}'.format(scipy.__version__))\n",
    "\n",
    "# Uncomment the following line(s) if you want to make figures with larger font labels,\n",
    "# for the 2-column format, and serif style labels (for consistency with the\n",
    "# ChainConsumer corner plots)\n",
    "\n",
    "# plt.rcParams['font.size'] = '16'\n",
    "# matplotlib.rcParams['font.family'] = 'serif'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preliminaries\n",
    "\n",
    "opz = cd.OPZ\n",
    "R_Newt = 10*u.km\n",
    "M_NS =  cd.M_NS\n",
    "R_NS = cd.solve_radius(M_NS, R_Newt)\n",
    "opz = cd.redshift(M_NS, R_NS)\n",
    "g = cd.g(M_NS, R_NS)\n",
    "\n",
    "print ('Calculations & simulations for:\\n  M_NS = {}, R_Newt = {}, R_GR = {:.3f}\\n  [ Check: g {:.3e} =? {:.3e} ]'.format(\n",
    "    M_NS, R_Newt, R_NS, g.value, cd.g(M_NS, R_Newt, Newt=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Code Architecture \n",
    "\n",
    "Here we demonstrate aspects of the Monte-Carlo approach to propagating errors. Quantities with errors are converted to a distribution by the `value_to_dist` function. Asymmetric distributions are generated by assuming that the first value is the maximum likelihood value.\n",
    "\n",
    "This approach can lead to inconsistencies if the provided quantities and errors are instead \"cumulative\" statistics, i.e. the 50th percentile and the (symmetric) 68% contours surrounding it.\n",
    "\n",
    "You can avoid this issue by pre-defining input values distributions using the `statistics=\"cumulative\"` flag for input into the other functions; this option is demonstrated below by generating an initial distribution for which the cumulative statistics are calculated, and then a second distribution from those values with the additional flag, which is consistent with the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# example (extremely asymmetric) quantity for demonstration\n",
    "ypar = (3.5, 0.3, 1.5) # interpreted as maximum likelihood value, lower error, upper error (1-sigma/68% assumed)\n",
    "\n",
    "# Generate the synthetic distribution matching the provided parameter inputs\n",
    "# x = cd.value_to_dist(ypar[:2])\n",
    "y = cd.value_to_dist(ypar, nsamp=10000)\n",
    "\n",
    "b = np.arange(0,12,0.1)\n",
    "with quantity_support():\n",
    "    # plt.hist(x.distribution, alpha=0.5, bins=b, density=True)\n",
    "    plt.hist(y.distribution, alpha=0.5, bins=b, density=True, label='target distribution')\n",
    "plt.xlim(1.5,9)\n",
    "plt.ylim(0,0.7)\n",
    "\n",
    "# overplot the inferred probability distribution (this for illustration only)\n",
    "\n",
    "_l = b <= ypar[0]\n",
    "p = np.zeros(len(b))\n",
    "p[_l] = norm.pdf(b[_l], loc=ypar[0], scale=ypar[1])*ypar[1]/(ypar[1]+ypar[2])*2\n",
    "p[~_l] = norm.pdf(b[~_l], loc=ypar[0], scale=ypar[2])*ypar[2]/(ypar[1]+ypar[2])*2\n",
    "plt.plot(b,p,'k--')\n",
    "\n",
    "# calculate the cumulative statistics, and generate a distribution from *those*\n",
    "# parameters, to illustrate the difference\n",
    "\n",
    "ypc = cd.intvl_to_errors(np.percentile(y.distribution, (50, 16, 84)))\n",
    "z = cd.value_to_dist(ypc, nsamp=10000) # cumulative errors for the (asymmetric) y-distribution\n",
    "\n",
    "print (\"Provided parameter range: {0}\\nDistribution cumulative statistics: ({1[0]:.4f}, {1[1]:.4f}, {1[2]:.4f})\".format(ypar, \n",
    "    tuple(ypc.value)))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "with quantity_support():\n",
    "    plt.hist(z.distribution, alpha=0.5, bins=b, density=True, label='ML distribution with cumulative stats')\n",
    "\n",
    "# Now set the flag for cumulative values to give the corresponding ML distribution\n",
    "\n",
    "y2 = cd.value_to_dist(ypc, nsamp=10000, statistics='cumulative')\n",
    "\n",
    "with quantity_support():\n",
    "    # plt.hist(x.distribution, alpha=0.5, bins=b, density=True)\n",
    "    plt.hist(y2.distribution, alpha=0.5, bins=b, density=True, label='corrected ML distribution')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Emission anisotropy\n",
    "\n",
    "Here we illustrate the various anisotropy factors as a function of system inclination. A call to the `anisotropy` routine with `test=True` will generate this plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_b, xi_p = cd.anisotropy(30*u.degree,test=True)\n",
    "\n",
    "print ('Example anisotropy values for i=30: xi_b = {:.4f}, xi_p = {:.4f}'.format(xi_b, xi_p))\n",
    "\n",
    "# plt.savefig(\"anisotropy.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Burst energetics and fuel composition\n",
    "\n",
    "Here we derive the expected ranges for the $\\alpha$-value for conservative burning, for a $1.4\\ M_\\odot$ neutron star with $R=11.6$ km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnuc_He = cd.Q_nuc(0.)\n",
    "qnuc_solar = cd.Q_nuc(0.7)\n",
    "\n",
    "qgrav = (const.c**2*(opz-1)/opz).to('MeV g-1')*const.m_p.to('g')\n",
    "\n",
    "print ('Q_nuc for H-rich material (X=0.7) is {:.4f}; Q_grav is {:.1f}'.format(qnuc_solar*u.MeV, qgrav))\n",
    "\n",
    "print ('Expected range of alpha-value (excluding anisotropy effects) is {:.1f}-{:.1f}'.format(\n",
    "    qgrav/(qnuc_solar*u.MeV), qgrav/(qnuc_He*u.MeV)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the average H-fraction at ignition $\\overline{X}$ is reduced compared to the accreted value $X_0$ by the effects of steady hot-CNO burning prior to ignition. Here we use equations 14-16 in the paper, to illustrate the relationship between the two, for various cases of recurrence time and $Z_\\mathrm{CNO}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of Xbar values\n",
    "\n",
    "nsamp = 100\n",
    "Xbar_arr = np.linspace(0.001,0.7,nsamp)\n",
    "Xbar_arr\n",
    "\n",
    "x_0 = np.zeros(nsamp)\n",
    "Z_CNO = 0.001 # CNO metallicity for the first line; we switch to \"solar\" after that\n",
    "\n",
    "dt_array = np.linspace(4.,20.,4)\n",
    "dt_array = np.insert(dt_array,0,4.)\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "\n",
    "# Loop over each of the recurrence time values, and plot the line\n",
    "\n",
    "for j, dt1 in enumerate(dt_array):\n",
    "    for i in range(nsamp):\n",
    "        x_0[i] = cd.X_0(Xbar_arr[i],Z_CNO,dt1*u.hr)\n",
    "\n",
    "    if j <= 1:\n",
    "        ax.plot(Xbar_arr, x_0, label='$\\Delta t$ = {:.0f} hr, $Z_\\mathrm{{CNO}}$ = {:.3f}'.format(\n",
    "            dt1, Z_CNO))\n",
    "    else:\n",
    "        ax.plot(Xbar_arr, x_0, label='$\\Delta t$ = {:.0f} hr'.format(dt1))\n",
    "\n",
    "    \n",
    "    Z_CNO = 0.02\n",
    "ax.set_xlabel(\"$\\overline{X}$\")\n",
    "ax.set_ylabel(\"$X_0$\")\n",
    "ax.axhline(0.77,linestyle='--',color='r')\n",
    "\n",
    "def Xbar(qnuc):\n",
    "    '''\n",
    "    Inverse function to Q_nuc, to generate the secondary axis\n",
    "    '''\n",
    "    \n",
    "    q = cd.Q_nuc(0.,quadratic=False, coeff=True)\n",
    "                 \n",
    "    return (qnuc-q[0])/q[1]\n",
    "\n",
    "secax = ax.secondary_xaxis('top', functions=(cd.Q_nuc, Xbar))\n",
    "secax.set_xlabel(\"$Q_\\mathrm{nuc}$ [MeV nucleon$^{-1}$]\")\n",
    "ax.legend(prop={\"size\":12})\n",
    "\n",
    "fig.set_size_inches(6,6)\n",
    "# fig.savefig(\"xbar.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Peak luminosity of a burst\n",
    "\n",
    "This section provides a simple introduction to the routines of the code and how incorporating the astrophysical uncertainties can affect the measurements. We adopt measurements of peak flux for a burst from 4U 0513+40 measured by _RXTE_, as part of the MINBAR sample ([Galloway et al. 2020](https://ui.adsabs.harvard.edu/abs/2020ApJS..249...32G)); the distance measurement for the host cluster NGC 1851 from [Watkins et al. (2015)](http://adsabs.harvard.edu/abs/2015ApJ...812..149W); and the lower limit on the system inclination from [Fiocchi et al. (2011)](http://adsabs.harvard.edu/abs/2011MNRAS.414L..41F).\n",
    "\n",
    "The warnings about `no bolometric correction applied` can be ignored, since we're dealing with (estimated) bolometric luminosities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (10.32, 0.24, 0.20) # asymmetric errors from Watkins et al. 2015; note that these are implicitly, the\n",
    "                        #   cumulative statistics, i.e. 50th, 15.9th and 84.1th percentiles\n",
    "imin, imax = 80, 90 # degrees assumed\n",
    "\n",
    "# Because we're dealing with cumulative stats, we pre-define the distance distribution here to make\n",
    "# sure it has the right shape\n",
    "# We use 10000 samples (1000 is the default) to get smoother distributions for final values;\n",
    "# this sets the size for all derived distributions\n",
    "\n",
    "ddist = cd.value_to_dist(d*u.kpc, statistics='cumulative', nsamp=10000)\n",
    "\n",
    "# Retrieve the burst measurements from MINBAR, if available\n",
    "\n",
    "id = 3443 # burst from 4U~0513-40 with RXTE/PCA on MJD 54043.68857 obsID 92403-01-15-04\n",
    "try:\n",
    "    import minbar as mb\n",
    "    has_minbar = True\n",
    "    \n",
    "    b = mb.Bursts()\n",
    "    F_pk, F_pk_err = b[id]['bpflux'], b[id]['e_bpflux']\n",
    "    name, time, obsid = b[id]['name'], b[id]['time'], b[id]['obsid']\n",
    "except:\n",
    "    has_minbar = False\n",
    "    F_pk, F_pk_err = 21.72, 0.6 # 1E-9 erg/cm^2/s bolometric\n",
    "    name, time, obsid = '4U 0513-40', 54043.68857, '92403-01-15-04'\n",
    "    \n",
    "print ('''Burst #{}, observed from {} on MJD {:.5f} (obsid {});\n",
    "  Peak (bolometric) flux is {:.2f} +/- {:.2f} E-9 erg/cm^2/s'''.format(\n",
    "    id, name, time, obsid, F_pk, F_pk_err))\n",
    "\n",
    "# Now calculate the various luminosities\n",
    "# In the absence of units for the flux, MINBAR units are assumed; in this case 1e-9 erg/cm^2/s\n",
    "l_iso = cd.luminosity( F_pk, dist=d[0], isotropic=True )\n",
    "\n",
    "l_asym = cd.luminosity( (F_pk, F_pk_err), dist=ddist, burst=True, imin=imin, imax=imax, \n",
    "                        fulldist=True)\n",
    "lc = l_asym['lum'].pdf_percentiles([50, 50 - cd.CONF / 2, 50 + cd.CONF / 2])\n",
    "\n",
    "l_unit = 1e38*u.erg/u.s\n",
    "print ('''\\nIsotropic luminosity is {:.2f}e38 erg/s\n",
    "  Taking into account anisotropy, ({:.2f}-{:.2f}+{:.2f})e38 erg/s'''.format(l_iso/l_unit, \n",
    "                                                                            lc[0]/l_unit, \n",
    "                                                (lc[0]-lc[1])/l_unit, (lc[2]-lc[0])/l_unit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot all the derived parameters, to demonstrate their codependence. \n",
    "\n",
    "Note also the additional clause when plotting the distribution objects, to provide the unit support; see https://docs.astropy.org/en/stable/uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html\n",
    "# https://www.kite.com/python/answers/how-to-set-the-spacing-between-subplots-in-matplotlib-in-python\n",
    "\n",
    "fig0, axs0 = plt.subplots(2, 2)\n",
    "fig0.set_size_inches(6,6)\n",
    "\n",
    "with quantity_support():\n",
    "    axs0[0, 1].hist(l_asym['lum'].distribution/1e38,bins=20,density=True, \n",
    "         label='Luminosity (erg/s)')\n",
    "    axs0[0, 1].axvline(l_iso,color='r',ls='--')\n",
    "    axs0[0, 1].axvline(lc[1],color='g',ls='--')\n",
    "    axs0[0, 1].axvline(lc[2],color='g',ls='--')\n",
    "    # axs0[0, 1].set_xlim((2.5e38,6.5e38))\n",
    "    axs0[0, 1].set_xlim((2.5,6.5))\n",
    "    axs0[0, 1].set_xlabel('Luminosity [$10^{38}$ erg/s]')\n",
    "    axs0[0, 1].axvline(lc[0],color='g')\n",
    "    \n",
    "    cfac = 0.09 # how far from the corner to put the labels\n",
    "    axs0[0, 1].text(1-cfac, 1-cfac, '(b)', horizontalalignment='center',\n",
    "      verticalalignment='center', transform=axs0[0,1].transAxes)\n",
    "\n",
    "    axs0[0,0].hist(l_asym['dist'].distribution, bins=20, density=True)\n",
    "    axs0[0,0].set_xlabel('Distance [kpc]')\n",
    "    \n",
    "    axs0[0, 0].text(1-cfac, 1-cfac, '(a)', horizontalalignment='center',\n",
    "      verticalalignment='center', transform=axs0[0,0].transAxes)\n",
    "\n",
    "    axs0[1,1].plot(l_asym['lum'].distribution/1e38, l_asym['i'].distribution, '.')\n",
    "    # axs0[1,1].set_xlim((2.5e38,6.5e38))\n",
    "    axs0[1, 1].sharex(axs0[0, 1])\n",
    "    axs0[1,1].set_xlabel('Luminosity [$10^{38}$ erg/s]')\n",
    "    axs0[1,1].set_ylabel('Inclination [degree]')\n",
    "\n",
    "    axs0[1, 1].text(cfac, 1-cfac, '(d)', horizontalalignment='center',\n",
    "      verticalalignment='center', transform=axs0[1,1].transAxes)\n",
    "\n",
    "    axs0[1,0].hist(l_asym['xi'].distribution, bins=20, density=True)\n",
    "    axs0[1,0].set_xlabel('Anisotropy factor')\n",
    "\n",
    "    axs0[1,0].text(1-cfac, 1-cfac, '(c)', horizontalalignment='center',\n",
    "      verticalalignment='center', transform=axs0[1,0].transAxes)\n",
    "\n",
    "fig0.tight_layout()\n",
    "# fig0.savefig(\"fig0.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Code validation\n",
    "\n",
    "As a preliminary test for the relations in the paper, we use the set of \"xbar\" Kepler runs as for [Goodwin et al. (2019)](https://ui.adsabs.harvard.edu/abs/2019ApJ...870...64G), which have a measured $\\overline{X}$ (mean H-fraction at ignition).\n",
    "\n",
    "These data are summarised in the `table1.mrt` file, supplied with the paper (and this repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = ascii.read('table1.mrt')#, format='mrt')\n",
    "\n",
    "plt.scatter(ft['mdot'],ft['Z'])\n",
    "plt.xlabel('$\\dot{m}$')\n",
    "plt.ylabel('$Z_\\mathrm{CNO}$')\n",
    "plt.title('$\\overline{X}$ Kepler model run grid')\n",
    "\n",
    "ft.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to test our Xbar calculation, to make sure equations 13 and 14 can replicate (approximately) what is measured from the Kepler models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead we can implement equations 13 and 14, neglecting the redshift factor, because\n",
    "# the t_CNO (along with the recurrence time) are all measured in the NS frame\n",
    "\n",
    "ft['t_CNO'] = 9.8*(ft['X']/0.7)/(ft['Z']/0.02) # hr\n",
    "\n",
    "ft['f_burn'] = ft['trec']/ft['t_CNO']\n",
    "ft['xbar_inf'] = ft['X']*(1-0.5*ft['f_burn'])\n",
    "\n",
    "exh = np.where(ft['trec'] > ft['t_CNO'])[0]\n",
    "ft['xbar_inf'][exh] = 0.5*ft[exh]['X']/ft[exh]['f_burn']\n",
    "\n",
    "# ... and our Xbar - X_0 function\n",
    "\n",
    "ft['X_inf'] = [cd.X_0(ft['Xbar'][i], ft['Z'][i], \n",
    "                           ft['trec'][i]*ft['trec'].unit) \n",
    "                       for i in range(len(ft))]\n",
    "\n",
    "# here identify the \"bad\" values of X_0_inf\n",
    "\n",
    "ex = abs(ft['X']-ft['X_inf']) > 0.075\n",
    "\n",
    "# ... and set up a colour array to plot them\n",
    "# https://kanoki.org/2020/08/30/matplotlib-scatter-plot-color-by-category-in-python/\n",
    "\n",
    "col = ['C1' if x else 'C0' for x in ex]\n",
    "\n",
    "# https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.set_size_inches(6,8)\n",
    "\n",
    "# Want to show the horizontal error bars to illustrate the  burst-to-burst variations\n",
    "# in the simulation results\n",
    "iex = np.where(~ex)[0]\n",
    "axs[0].errorbar(ft['Xbar'][iex],ft['xbar_inf'][iex],xerr=ft['e_Xbar'][iex],fmt='o')\n",
    "iex = np.where(ex)[0]\n",
    "axs[0].errorbar(ft['Xbar'][iex],ft['xbar_inf'][iex],xerr=ft['e_Xbar'][iex],fmt='o')\n",
    "\n",
    "axs[0].plot([0.0, 0.77], [0.0, 0.77], 'k--')\n",
    "axs[0].set_xlabel('Model $\\overline{X}$')\n",
    "axs[0].set_ylabel('Inferred $\\overline{X}$')\n",
    "# plt.ylim(-0.4, 0.79)\n",
    "    \n",
    "# quantify the error here\n",
    "\n",
    "xbar_rms = np.sqrt(np.mean((ft['Xbar']-ft['xbar_inf'])**2))\n",
    "print ('Xbar RMS error: {:.4f}'.format(\n",
    "    xbar_rms))\n",
    "loX = np.where(ft['Xbar'] < 0.13)[0]\n",
    "print ('  for low Xbar: {:.4f}'.format(\n",
    "    np.sqrt(np.mean((ft['Xbar'][loX]-ft['xbar_inf'][loX])**2))))\n",
    "\n",
    "# Second plot\n",
    "\n",
    "axs[1].scatter(ft['X'], ft['X_inf'], c=col)\n",
    "\n",
    "axs[1].plot([0.2, 0.8], [0.2, 0.8], 'k--')\n",
    "axs[1].set_xlabel('Model $X_0$')\n",
    "axs[1].set_ylabel('Inferred $X_0$')\n",
    "axs[1].set_ylim(0.15,1.05)\n",
    "\n",
    "# Example adapted from\n",
    "# https://stackoverflow.com/questions/14432557/matplotlib-scatter-plot-with-different-text-at-each-data-point\n",
    "\n",
    "for i in np.where(ex)[0]:\n",
    "    # print (i, full_table.iloc[i]['run no.'], full_table.iloc[i]['X'], full_table.iloc[i]['X_inf'])\n",
    "    axs[1].annotate(' {}'.format(ft['run'][i]), \n",
    "        (ft['X'][i]+0.01, ft['X_inf'][i]))\n",
    "    \n",
    "# quantify the error here\n",
    "\n",
    "excl = np.where((ft['run'] < 57) | (ft['run'] > 60))[0]\n",
    "X_0_rms = np.sqrt(np.mean((ft['X'][excl]-ft['X_inf'][excl])**2))\n",
    "print ('X_0 RMS error, excluding outliers: {:.4f}'.format(\n",
    "    X_0_rms))\n",
    "\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"validation.pdf\", bbox_inches='tight')\n",
    "\n",
    "ft[['X','Z','mdot','n_burst','Xbar','e_Xbar','trec']][loX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agreement between the $\\overline{X}$ inferred from equations 13 and 14, and measured from the model outputs, is fairly good, similar for the $X_0$. The four most significant outliers are at the low-accretion rate (0.1 mdot_Edd) and high Z_CNO (0.1), but also have unusually large scatter in the recurrence times (and hence $\\overline{X}$ values; highlighted in orange).\n",
    "\n",
    "The issue is not just because they get to low values of $\\overline{X}$, as other runs get to smaller values but have more consistent inferred values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.sort('Xbar')\n",
    "ft[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a more extensive and realistic simulation, choosing randomly-distributed inclinations, looping over the available models, and generating some random samples for each one\n",
    "\n",
    "We keep redshift and radius fixed, although for full realism you might also draw from distributions on both these parameters also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = len(ft)\n",
    "n_per_run = 10\n",
    "\n",
    "# Arbitrary (but realistic) distance and bolometric correction\n",
    "\n",
    "d = 6*u.kpc\n",
    "bol_corr = 1.4\n",
    "\n",
    "# Generate an array of inclinations\n",
    "\n",
    "i = cd.iso_dist(nsamp=n_per_run*n_rows)\n",
    "\n",
    "# Define the result array\n",
    "# columns are run ID, X_sim, inclination, X_meas, X_meas_lo, X_meas_hi]\n",
    "\n",
    "res_arr = np.zeros((6,n_per_run*n_rows))\n",
    "res_arr[2,:] = i.distribution\n",
    "\n",
    "ft.sort('run')\n",
    "for i in range(n_rows):\n",
    "    print ('Simulating run id {}/{}...'.format(ft['run'][i]+1, n_rows))\n",
    "    \n",
    "    # Now transform the data into observations, given a distance, redshift and inclination\n",
    "    # Recurrence time\n",
    "\n",
    "    tdel = (ft['trec'][i], ft['e_trec'][i])*u.hr\n",
    "    tdel *= opz\n",
    "\n",
    "    mdot_Edd = 8.8e4*1.7/(1+ft['X'][i])*u.g/u.cm**2/u.s\n",
    "    for j in range(n_per_run):\n",
    "        \n",
    "        res_arr[0,i*n_per_run+j] = ft['run'][i]\n",
    "        res_arr[1,i*n_per_run+j] = ft['X'][i]\n",
    "        \n",
    "        # Fluence transforms the same way as flux, but we have to add (and then remove) \n",
    "        # seconds as a unit to get the right dimensions\n",
    "\n",
    "        e_b = cd.lum_to_flux( (ft[\"fluen\"][i]*1e38, \n",
    "                               ft['e_fluen'][i]*1e38)*u.erg/u.s, dist=d, burst=True, dip=False,\n",
    "                        isotropic=False, inclination=res_arr[2,i*n_per_run+j], fulldist=True)\n",
    "    \n",
    "        e_b = e_b[\"flux\"].distribution*u.s\n",
    "\n",
    "        # Persistent flux\n",
    "        # Need to work out what mdot is in real physical units to use this\n",
    "\n",
    "        mdot = (ft['mdot'][i]*mdot_Edd*4*math.pi*R_NS**2).to('g s-1')\n",
    "\n",
    "        fper = cd.fper(mdot, (d, res_arr[2,i*n_per_run+j], opz, 0.0), c_bol=bol_corr)\n",
    "\n",
    "        xbar = cd.hfrac(tdel, fper=fper, fluen=e_b, c_bol=bol_corr,\n",
    "                  isotropic=False, inclination=None, fulldist=True)\n",
    "\n",
    "        # save results; now X_0 might have -1 values, so trap those here\n",
    "        \n",
    "        gz = np.where(xbar['X_0'].distribution >= 0.)[0]\n",
    "        if len(gz) > 2:\n",
    "        \n",
    "            xpc = np.percentile(xbar['X_0'].distribution[gz],(50,16,84))\n",
    "            res_arr[3,i*n_per_run+j] = xpc[0]\n",
    "            res_arr[4,i*n_per_run+j] = xpc[0]-xpc[1]\n",
    "            res_arr[5,i*n_per_run+j] = xpc[2]-xpc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we convert to a DataFrame, we can do some more useful stuff\n",
    "# But we first need to also convert the simulation table to a DataFrame\n",
    "\n",
    "ft_df = ft.to_pandas()\n",
    "ra = pd.DataFrame(res_arr.T, columns=['run', 'X', 'inclination', 'X_meas', 'X_meas_lo', 'X_meas_hi'])\n",
    "\n",
    "# and merge the two tables on the common \"run\" index \n",
    "\n",
    "ra = pd.merge(ra, ft_df, on='run')\n",
    "\n",
    "# Now plot the results\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(6,6)\n",
    "colors = {0.005: 'C0', 0.02: 'C1', 0.1: 'C2'}\n",
    "\n",
    "# plot grouped by the CNO metallicity\n",
    "\n",
    "grouped = ra.groupby('Z')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='X_x', y='X_meas', label=key, color=colors[key])\n",
    "ax.legend(title='$Z_\\mathrm{CNO}$')\n",
    "ax.plot([0.2, 0.8], [0.2, 0.8], 'k--')\n",
    "ax.set_xlabel('Model $X_0$')\n",
    "ax.set_ylabel('Inferred $X_0$')\n",
    "\n",
    "X_0_rms = np.sqrt(np.mean((res_arr[1,:]-res_arr[3,:])**2))\n",
    "print ('X_0 RMS error: {:.4f}'.format(\n",
    "    X_0_rms))\n",
    "\n",
    "# Now the Z_CNO range is pretty huge (factor of 20). Try to restrict to the \n",
    "# 0.005-0.02 values here to get a more realistic measure of success\n",
    "\n",
    "loZ = ra['Z'] < 0.1\n",
    "\n",
    "X_0_rms_loZ = np.sqrt(np.mean((ra['X_x'][loZ]-ra['X_meas'][loZ])**2))\n",
    "print ('X_0 RMS error low Z: {:.4f}'.format(\n",
    "    X_0_rms_loZ))# Now plot the results\n",
    "\n",
    "# plt.savefig(\"X0.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to demonstrate the achievable precision is the fractional error distributions as a function of Z_CNO mismatch, split for the \"low Z\" dat ($Z_{\\rm CNO}=0.005, 0.02$) and the \"high Z\" ($Z_{\\rm CNO}=0.1$). \n",
    "\n",
    "Since we assume $Z_{\\rm CNO}$ is 0.02 across the board, the mismatch with the \"high Z\" group is greatest (and the distribution of fractional errors are also higher on average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra['fracerr'] = abs(ra['X_x']-ra['X_meas'])/ra['X_x']\n",
    "plt.hist(ra['fracerr'][loZ],bins=np.arange(20)/20.,label='$Z_\\mathrm{CNO}=0.005, 0.02$')\n",
    "plt.hist(ra['fracerr'][~loZ],bins=np.arange(20)/20.,alpha=0.5,label='$Z_\\mathrm{CNO}=0.1$')\n",
    "\n",
    "plt.xlabel('Fractional error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Two or more regular, consistent bursts\n",
    "\n",
    "For the three sources in the reference burst sample ([Galloway et al. 2017](http://adsabs.harvard.edu/abs/2017PASA...34...19G)), we have moderately good constraints on the composition; so test how well the current analyses reproduce those values here\n",
    "\n",
    "First read in the data from the reference burst sample; this is done as part of `concord`, where we define an arbitrary `ObservedBurst` object to trigger reading in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't matter which burst you choose here; this is just\n",
    "# to trigger concord to read the reference burst table\n",
    "\n",
    "obs = cd.ObservedBurst.ref('GS 1826-24', 3.5)\n",
    "\n",
    "i = 0 # select the first epoch burst from 1826\n",
    "obs.table[i:i+3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GS 1826-24, [Meisel et al. (2018)](https://ui.adsabs.harvard.edu/abs/2018ApJ...860..147M) suggested an inclination of 65 or 80 deg; [Johnston et al. (2020)](https://ui.adsabs.harvard.edu/abs/2020MNRAS.494.4576J) found $i = 69^{+2}_{-3}$ degrees. Lack of dipping would suggest its $\\lesssim75$\n",
    "\n",
    "Below we calculate the $\\alpha$-value via `concord` for comparison with the tabled value, which was calculated in a slightly different way originally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (obs.table[i])\n",
    "\n",
    "tdel = cd.decode_LaTeX(obs.table[i][3])\n",
    "fper, e_fper = cd.decode_LaTeX(obs.table[i][4])\n",
    "fluen, fluene  = cd.decode_LaTeX(obs.table[i][7])\n",
    "bc, bce = cd.decode_LaTeX(obs.table[i][5])\n",
    "\n",
    "alpha = cd.alpha(tdel, (fluen, fluene), (fper, e_fper), c_bol=(bc, bce), \n",
    "                    fulldist=True,nsamp=10000)\n",
    "\n",
    "with quantity_support():\n",
    "    plt.hist(alpha['alpha'].distribution,bins=20)\n",
    "    \n",
    "result = cd.intvl_to_errors( np.percentile(alpha['alpha'].distribution, [50, 50-cd.CONF/2., 50+cd.CONF/2.]))\n",
    "\n",
    "print (\"concord alpha-value for epoch {} is {:.2f}_{:.2f}^{:.2f}\".format(\n",
    "    obs.table[i][0],result[0],-result[1],result[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can generate the $X_0$ values for each epoch, and plot them as a simple histogram (`histogram = True`), or demonstrating the dependence also on the inclination (`histogram = False`; latter is the plot in the paper, Figure 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the three epochs and determine the X_0 distribution for each one, comparing with\n",
    "# Johnston et al. 20\n",
    "\n",
    "histogram = False # can switch between histogram and scatter plot\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "\n",
    "Z_CNO=0.02\n",
    "for i in range(3):\n",
    "    tdel = cd.decode_LaTeX(obs.table[i][3])\n",
    "    fper, e_fper = cd.decode_LaTeX(obs.table[i][4])\n",
    "    fluen, fluene  = cd.decode_LaTeX(obs.table[i][7])\n",
    "    bc, bce = cd.decode_LaTeX(obs.table[i][5])\n",
    "\n",
    "    xbar = cd.hfrac(tdel*u.hr, fluen=(fluen, fluene), fper=(fper, e_fper), c_bol=(bc, bce), \n",
    "                    imin=66, imax=71, zcno=Z_CNO,\n",
    "                    fulldist=True)#, imin=65, imax=80)\n",
    "\n",
    "    if histogram:\n",
    "        plt.hist(xbar['X_0'].distribution,alpha=0.5,label=obs.table[i][0],bins=np.arange(60)/60.,density=True)\n",
    "    else:\n",
    "        plt.scatter(xbar['X_0'].distribution,xbar['i'].distribution,label=obs.table[i][0],s=5)\n",
    "    \n",
    "    print (tdel, fluen, fper, (bc, bce), np.percentile(xbar['X_0'].distribution, (50-cd.CONF/2, 50, 50+cd.CONF/2)))\n",
    "\n",
    "plt.xlabel('$X_0$')\n",
    "plt.axvline(0.74,color='r',label='Johnston et al. (2020)') # central value from johnston20\n",
    "plt.axvline(0.74-0.03,color='r',linestyle='--')\n",
    "plt.axvline(0.74+0.02,color='r',linestyle='--')\n",
    "plt.xlim(0.4,0.8)\n",
    "if histogram:\n",
    "    plt.ylim(0,11)\n",
    "else:\n",
    "    plt.ylabel('Inclination [deg]')\n",
    "plt.legend(loc='lower left',prop={\"size\":12})\n",
    "\n",
    "# plt.savefig(\"fig1826.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot suggests that the code underestimates the fuel fraction by a few tens of percent, compared to the more detailed comparison to Kepler models. However, that comparison also finds $Z_{\\rm CNO}\\approx0.01$, compared to the 0.02 that we assume. Unfortunately reducing the CNO mass fraction gives us also reduced $X$, so there remains a discrepancy between our results and those of [Johnston et al. (2020)](https://ui.adsabs.harvard.edu/abs/2020MNRAS.494.4576J)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For one of the epochs, experiment with the variation in Z\n",
    "\n",
    "i=0\n",
    "print (obs.table[i][3], obs.table[i][4], obs.table[i][5], obs.table[i][7])\n",
    "\n",
    "for Z_CNO in (0.001, 0.02, 0.03):\n",
    "    tdel = cd.decode_LaTeX(obs.table[i][3])\n",
    "    fper, e_fper = cd.decode_LaTeX(obs.table[i][4])\n",
    "    fluen, fluene  = cd.decode_LaTeX(obs.table[i][7])\n",
    "    bc, bce = cd.decode_LaTeX(obs.table[i][5])\n",
    "\n",
    "    xbar = cd.hfrac(tdel*u.hr, fluen=(fluen, fluene), fper=(fper, e_fper), c_bol=(bc, bce), \n",
    "                    imin=66, imax=71, zcno=Z_CNO,\n",
    "                    fulldist=True)#,debug=True)#, imin=65, imax=80)\n",
    "    plt.hist(xbar['X_0'].distribution,alpha=0.5,bins=np.arange(60)/60.,density=True, label='{:.3f}'.format(Z_CNO))\n",
    "\n",
    "    print (tdel, fluen, fper, (bc, bce), np.percentile(xbar['X_0'].distribution, (50-cd.CONF/2, 50, 50+cd.CONF/2)))\n",
    "\n",
    "plt.xlabel('$X_0$')\n",
    "plt.xlim(0.4,0.8)\n",
    "plt.ylim(0,11)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 One (or more) bursts, no recurrence times\n",
    "\n",
    "Here we analyse the bursts from XMMU J181227.8-181234 observed by _RXTE_, as also analysed by [Goodwin et al. (2019)](https://ui.adsabs.harvard.edu/abs/2019MNRAS.486.4149G). A total of 6 bursts were observed over a total exposure time of 0.3446 d, with low duty cycle and hence many gaps in between. \n",
    "\n",
    "So we compute a distribution of mean recurrence times consistent with the measurements, and propagate this distribution through to the rest of the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations\n",
    "\n",
    "exp = (0.3446*u.d).to('hr') # total exposure\n",
    "nburst = 6 # total number of bursts observed (excluding SWT bursts)\n",
    "\n",
    "fluen, fluene = 0.0241, 0.0073 # 1e-6 erg/cm^2\n",
    "pflux, pfluxe = 1.19, 0.16 # 1e-9 erg/cm^2/s\n",
    "bc = 2.522\n",
    "f_unit = 1e-9*u.erg/u.cm**2\n",
    "\n",
    "# Assumptions here\n",
    "\n",
    "opz_def = cd.redshift(1.4*const.M_sun,11.2*u.km)\n",
    "\n",
    "xmax = 0.77\n",
    "zcno_def = 0.02\n",
    "\n",
    "# Because the ultimate acceptance fraction is so small, you need to start with \n",
    "# many many samples to end up with anything to plot at the end\n",
    "\n",
    "# nbin=1000000 # for paper plot\n",
    "nbin=2000 # for testing (faster)\n",
    "\n",
    "z_def = opz_def-1.\n",
    "print (\"Assumed M_NS = {}, R_NS = {:.4f}, 1+z = {:.4f}, Z_CNO = {}\".format(\n",
    "    1.4*u.M_sun,11.2*u.km,1.+z_def,zcno_def))\n",
    "\n",
    "# Estimate tdel and error based on Poisson stats\n",
    "    \n",
    "tdel_arr = cd.tdel_dist(nburst, exp, nsamp=nbin)\n",
    "tdel = exp/nburst\n",
    "\n",
    "lim = np.percentile(tdel_arr.distribution,(16,50,84))\n",
    "print ('Synthetic distribution of tdel has {:.3f}_{:.3f}^{:.3f}'.format(\n",
    "    tdel.value,(lim[0]-tdel).value,lim[2]-tdel))\n",
    "\n",
    "with quantity_support():\n",
    "    plt.hist(tdel_arr.distribution,bins=20,density=True)\n",
    "    plt.xlabel('Inferred recurrence time [h]')\n",
    "    plt.axvline(tdel,linestyle='--',color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate alpha and plot the histogram, using the concord routine\n",
    "\n",
    "alpha = cd.alpha(tdel_arr,(fluen,fluene),(pflux,pfluxe),bc, fulldist=True)\n",
    "alpha['alpha'].distribution\n",
    "\n",
    "assert cd.len_dist(tdel_arr) == cd.len_dist(alpha['alpha'])\n",
    "with quantity_support():\n",
    "    plt.hist(alpha['alpha'].distribution,bins=np.arange(0,2000,50),density=True)\n",
    "plt.xlabel('alpha')\n",
    "plt.xlim(100,2000)\n",
    "\n",
    "# Also show the statistics\n",
    "\n",
    "ap = np.percentile(alpha['alpha'].distribution,(16,50,84))\n",
    "print (\"alpha = {:.2f} (68% range {:.2f}-{:.2f})\".format(ap[1],ap[0],ap[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the H-fraction at ignition, and the corresponding fuel composition\n",
    "# and system inclination. Make sure to include fulldist=True to get the distributions\n",
    "\n",
    "if (cd.len_dist(tdel_arr) > 2000):\n",
    "    print ('Calculating Xbar for {} samples, this may take some time...'.format(cd.len_dist(tdel_arr)))\n",
    "    \n",
    "# Initial version of this calculation just used the fixed value of Z_CNO; but here\n",
    "# we follow the earlier analysis by having a randomly distributed set of values\n",
    "\n",
    "Z_CNO = np.random.random(nbin)*0.02\n",
    "\n",
    "# print (type(tdel_arr), type(alpha['alpha']), cd.len_dist(tdel_arr), tdel_arr, cd.len_dist(alpha['alpha']), alpha['alpha'])\n",
    "xbar = cd.hfrac(tdel_arr,alpha['alpha'],opz=1+z_def,\n",
    "                # zcno=zcno_def,\n",
    "                zcno=Z_CNO,\n",
    "                isotropic=False,fulldist=True)#,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the results\n",
    "\n",
    "plt.hist(xbar['X_0'].distribution,bins=np.arange(0.,0.3,0.02), density=True)\n",
    "plt.xlim(-0.1,0.8)\n",
    "plt.xlabel('inferred X_0')\n",
    "plt.axvline(x=xmax,linestyle='--',color='k')\n",
    "plt.axvline(x=0,linestyle='--',color='k')\n",
    "\n",
    "good=np.where(np.logical_and(xbar['X_0'].distribution <= xmax,xbar['X_0'].distribution > 0))[0]\n",
    "print (len(good), cd.len_dist(alpha['alpha']))\n",
    "\n",
    "# Stats on the accepted samples\n",
    "\n",
    "print (\"Accepting {}% of samples\".format(\n",
    "    100.*len(good)/cd.len_dist(alpha['alpha'])))\n",
    "\n",
    "# Display results\n",
    "xp = np.percentile(xbar['X_0'].distribution[good],(50,16,84,95))\n",
    "print (\"For assumed 1+z = {:.4f}, Z_CNO = {:.3f}, X_0 = {:.4f}_{:.4f}^{:.4f} (< {:.4f} @ 95%)\".format(\n",
    "    1+z_def,zcno_def,xp[0], xp[1], xp[2], xp[3]))\n",
    "ip = np.percentile(xbar['i'].distribution,(50,16,84))\n",
    "print (\"i = {:.1f}_{:.1f}^{:.1f}\".format( ip[0].value, ip[1].value, ip[2]))\n",
    "\n",
    "with quantity_support():\n",
    "    # \"bad\" values are returned as -1, so no need to highlight the \"good\"\n",
    "    plt.plot(xbar['X_0'].distribution,xbar['i'].distribution,'.', alpha=0.1)\n",
    "    plt.xlim(0,0.2)\n",
    "    plt.xlabel('$X_0$')\n",
    "    plt.ylabel('inclination')\n",
    "    # plt.plot(_x_0.distribution[good],i.distribution[good],'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and display, copying the approach from the previous source\n",
    "\n",
    "data = np.vstack((xbar['xbar'].distribution[good], \n",
    "                  xbar['X_0'].distribution[good], \n",
    "                  Z_CNO[good]*100, \n",
    "                  tdel_arr.distribution[good].value)).T\n",
    "labels = [\"$\\overline{X}$\", \"$X_0$\", \"$Z_\\mathrm{CNO}$\", \"$\\Delta t$\"]\n",
    "# labels = ['X', '$X_0$', '$Z_\\mathrm{CNO}$', '$Delta t$']\n",
    "\n",
    "# not sure if this is necessary\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# If you pass in parameter labels and only one chain, you can also get parameter bounds\n",
    "# kde=True will set smoothing on the chains, which you might not want\n",
    "# the statistics configuration is to ensure consistency with the other stats derived\n",
    "# in this notebook and the paper\n",
    "c = ChainConsumer().add_chain(data, parameters=labels, kde=False).configure(statistics=\"cumulative\")\n",
    "\n",
    "fig = c.plotter.plot(figsize=(6,6)\n",
    "# uncomment the following line to save the plot\n",
    "                                # ,filename='xmmuj181227.8-181234.pdf'\n",
    "                                )\n",
    "\n",
    "print (matplotlib.rcParams['font.family'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 One burst – estimating the distance and burst rate\n",
    "\n",
    "Here wefollow through the analysis of the (single) burst observed with _INTEGRAL_/JEM-X from IGR J17591-2342, with parameters extracted from Table 3 of [Kuiper et al. 2020](https://ui.adsabs.harvard.edu/abs/2020A%26A...641A..37K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'IGR J17591-2342'\n",
    "t0 = 58380.96358 # MJD\n",
    "instr = 'IJ1'\n",
    "\n",
    "F_pk, F_pk_err = 76., 14. # 1e-9 erg/cm^2/s\n",
    "F_pers, F_pers_err = 1.2, 0.2 # 1e-9 erg/cm^2/s\n",
    "E_b, E_b_err = 1.1, 0.1 # 1e-6 erg/cm^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a rough limit of 28-30 degrees for the inclination\n",
    "# We can use that to constrain the various system parameters we've derived\n",
    "# Use 100000 samples to make the figure for publication, with slightly smoother\n",
    "# distributions\n",
    "# Below we also illustrate two different ways to provide the units on the flux\n",
    "\n",
    "dist_theo = cd.dist((F_pk, F_pk_err)*flux_unit*1e-9, empirical=False, fulldist=True, \n",
    "                    imin=24, imax=30, nsamp=1000)\n",
    "\n",
    "dist2 = cd.dist((F_pk, F_pk_err), empirical=True, fulldist=True, \n",
    "                imin=24, imax=30, nsamp=1000)\n",
    "\n",
    "fig2 = plt.figure()\n",
    "fig2.set_size_inches(6,6)\n",
    "\n",
    "bins = np.arange(1,13,0.2)\n",
    "with quantity_support():\n",
    "    plt.hist(dist2['dist'].distribution,density=True,bins=bins,\n",
    "             label=\"Empirical $L_\\mathrm{Edd}$\\n(Kuulkers et al. 2003)\")\n",
    "\n",
    "    plt.hist(dist_theo['dist'].distribution,density=True,bins=bins,alpha=0.7,\n",
    "            label='Theoretical ($M=1.4 M_\\odot$,\\n$1+z=1.259$)')\n",
    "    \n",
    "    plt.xlabel('Distance [kpc]')\n",
    "    plt.xlim((5,13))\n",
    "dper = np.percentile(dist2['dist'].distribution,(50,16,84))\n",
    "plt.axvline(dper[0],color='g', label='Kuiper et al. (2020)')\n",
    "plt.axvline(dper[1],color='g',ls='--')\n",
    "plt.axvline(dper[2],color='g',ls='--')\n",
    "# xiper = np.percentile(dist2['xi_b'][g],(50,16,84))\n",
    "\n",
    "# These values for the paper\n",
    "\n",
    "print ('''Distance limits including inclination constraint {:.2f}_{:.2f}^{:+.2f} kpc ({} samples);\n",
    "  with burst anisotropy factor averaging xi_b = {:.2f}'''.format(\n",
    "    dper[0], dper[1]-dper[0], dper[2]-dper[0], len(dist2['dist'].distribution),\n",
    "    np.mean(dist2['xi_b'].distribution)))\n",
    "\n",
    "plt.legend(title=src, prop={\"size\":12})\n",
    "\n",
    "# fig2.savefig(\"fig2.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the accretion rate\n",
    "\n",
    "m2 = cd.mdot((F_pers,F_pers_err),dist2['dist'],inclination=dist2['i'],fulldist=True)#,nsamp=2000)\n",
    "\n",
    "mdot = np.percentile(m2['mdot'].distribution,(50,16,84))\n",
    "print ('''Accretion rate implied from persistent flux, with xi_p = {:.2f} (average) is {:.0f}_{:.0f}^{:+.0f} \n",
    "  or {:.3f}_{:.3f}^{:+.3f} in units of the Eddington accretion rate'''.format(\n",
    "    np.mean(m2['xi_p'].distribution),\n",
    "    mdot[0].value,(mdot[1]-mdot[0]).value,mdot[2]-mdot[0], #m2['mdot'].unit,\n",
    "    (mdot[0]/mdot_Edd).value,((mdot[1]-mdot[0])/mdot_Edd).value,(mdot[2]-mdot[0])/mdot_Edd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation determines the ignition column; in the absence of a value for $\\overline{X}$, a random distribution of values is used. A uniform distribution of $\\overline{X}$ is perhaps not realistic, but it's substantially more work to do better. Also, large values of $\\overline{X}$ are suppressed by the limit on $X_0$, down the track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the distribution has to be consistent with the distance array\n",
    "nsamp = cd.len_dist(dist2['dist'])\n",
    "# print (nsamp)\n",
    "\n",
    "Xbar = unc.uniform(lower=0.0*u.dimensionless_unscaled, upper=0.7*u.dimensionless_unscaled,\n",
    "                  n_samples=nsamp)\n",
    "\n",
    "# Calculate the ignition column\n",
    "\n",
    "y2 = cd.yign((E_b, E_b_err), dist2['dist'], inclination=dist2['i'], \n",
    "             Xbar=Xbar, fulldist=True)\n",
    "np.shape(y2['yign'].distribution)\n",
    "\n",
    "# Now the recurrence time\n",
    "\n",
    "dt1 = y2['yign']/m2['mdot']*cd.OPZ\n",
    "\n",
    "# The combination of Xbar and recurrence time constrain X_0 and Z_CNO. \n",
    "# For each pair, we calculate the values here\n",
    "\n",
    "Z_CNO = np.random.random(nsamp)*0.02\n",
    "\n",
    "x_0 = np.zeros(nsamp)\n",
    "for i in range(nsamp):\n",
    "    # print (Xbar.distribution[i],Z_CNO[i])#,dt1.distribution[i].to('hr'))\n",
    "    x_0[i] = cd.X_0(Xbar.distribution[i],Z_CNO[i],dt1.distribution[i].to('hr'))\n",
    "\n",
    "# We restrict to physical values i.e. x_0 < 0.75\n",
    "\n",
    "g = np.where(x_0 < 0.75)[0]\n",
    "\n",
    "print ('Acceptance fraction {:.2f}% of {} samples'.format(100*len(g)/len(x_0), len(x_0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can display all the values of yign, and dt1 that meet the criterion on x_0 imposed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results for the realistic subset of the distributions\n",
    "\n",
    "yign = np.percentile(y2['yign'].distribution[g]/1e8,(50,16,84))\n",
    "print('''\n",
    "Ignition column is {:.2f}_{:.2f}^{:+.2f} e8 {}'''.format(\n",
    "    yign[0].value,(yign[1]-yign[0]).value,(yign[2]-yign[0]).value,y2['yign'].unit))\n",
    "\n",
    "dtc = np.percentile(dt1.distribution[g].to('d').value, (50,16,84))\n",
    "print('''\n",
    "Inferred recurrence time is {:.2f}_{:.2f}^{:+.2f} d'''.format(\n",
    "    dtc[0], dtc[1]-dtc[0], dtc[2]-dtc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ugly but informative plot\n",
    "\n",
    "fig3, axs3 = plt.subplots(2, 2)\n",
    "with quantity_support():\n",
    "    axs3[0,0].hist(x_0, density=True)\n",
    "    axs3[0,0].set_title('$X_0$')\n",
    "    axs3[0, 0].axvline(0.75,color='r',linestyle='--')\n",
    "\n",
    "    axs3[0, 1].hist(m2['mdot'].distribution[g], bins=20, density=True)\n",
    "    axs3[0, 1].set_title('$\\dot{m}$')\n",
    "    \n",
    "    axs3[1, 0].hist(y2['yign'].distribution[g], bins=20, density=True)\n",
    "    axs3[1, 0].set_title('$y_\\mathrm{ign}$')\n",
    "    \n",
    "    axs3[1, 1].hist(dt1.distribution[g].to('hr'), bins=20, density=True)\n",
    "    axs3[1, 1].set_title('$\\Delta t$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using instead chainconsumer to plot; see \n",
    "# https://samreay.github.io/ChainConsumer/examples/customisations/plot_one_chain.html\n",
    "\n",
    "data = np.vstack((Xbar.distribution[g].value, \n",
    "                  x_0[g], \n",
    "                  Z_CNO[g]*100.,  # Z_CNO as a percentage, to reduce the size of the label\n",
    "                  dt1.distribution[g].to('d').value)).T\n",
    "labels = [\"$\\overline{X}$\", \"$X_0$\", \"$Z_\\mathrm{CNO}$\", \"$\\Delta t$\"]\n",
    "\n",
    "# not sure if this is necessary\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# If you pass in parameter labels and only one chain, you can also get parameter bounds\n",
    "fig = ChainConsumer().add_chain(data, \n",
    "                                parameters=labels).configure(statistics=\"cumulative\").plotter.plot(figsize=(6,6)\n",
    "# uncomment the following line to save the plot\n",
    "                                # ,filename='igrJ17591-2342.pdf'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Zero bursts  – constraining  the distance\n",
    "\n",
    "Here we compare the results from a study of the accretion-powered millisecond pulsar IGR J00291+5934 [(Galloway et al. 2006)](http://arxiv.org/abs/astro-ph/0604345) with the subsequent detection of a burst from the source [(Kuin et al. 2015)](http://www.astronomerstelegram.org/?read=7849), with distance derivation (via the `dist` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'IGR J00291+5934'\n",
    "F_pk, F_pk_err = 18, 4 # 1e-8 erg/cm^2/s; \n",
    "\n",
    "gal06b = (3.9, 5.2) # llim, ulim (@ 3-sigma), for X=0.7\n",
    "\n",
    "# Generate the distance distributions based on the peak flux, and the\n",
    "# empirical Eddington luminosity; two options here, the first with no\n",
    "# constraints on the inclination, the second imposing the inferred\n",
    "# inclination range of torres08, of 22-32 degrees\n",
    "\n",
    "dist_all = cd.dist((F_pk*1e-8, F_pk_err*1e-8)*flux_unit, empirical=True, fulldist=True)\n",
    "\n",
    "dist = cd.dist((F_pk*1e-8, F_pk_err*1e-8)*flux_unit, empirical=True, fulldist=True, imin=22, imax=32)#, plot=True)\n",
    "\n",
    "# Display the best estimate of the plausible range\n",
    "\n",
    "dper = np.percentile(dist['dist'].distribution,(50,16,84))\n",
    "print ('''Distance range including inclination constraints is {:.2f}_{:.2f}^{:.2f} kpc\n",
    "  incorporates model-predicted anisotropy averaging xi_b = {:.3f}'''.format(\n",
    "    dper[0],(dper[1]-dper[0]),dper[2]-dper[0],np.mean(dist['xi_b'].distribution)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to compare the 2 distributions\n",
    "\n",
    "fig1 = plt.figure()\n",
    "fig1.set_size_inches(6,6)\n",
    "with quantity_support():\n",
    "    plt.hist(dist_all['dist'].distribution,bins=np.arange(40)/5.,density=True, \n",
    "         label='$i<{:.0f}$'.format(cd.IMAX_NDIP))\n",
    "\n",
    "    plt.hist(dist['dist'].distribution,bins=np.arange(40)/5.,alpha=0.7,density=True,\n",
    "        label='${:.0f}<i<{:.0f}$'.format(22,32))\n",
    "plt.xlim(2.5,9.4)\n",
    "plt.xlabel('Distance (kpc)')\n",
    "\n",
    "plt.axvline(dper[0],color='g')\n",
    "plt.axvline(dper[1],color='g',ls='--')\n",
    "plt.axvline(dper[2],color='g',ls='--')\n",
    "\n",
    "plt.axvline(gal06b[0],color='r',ls='--',label='Galloway (2006)')\n",
    "plt.axvline(gal06b[1],color='r',ls='--')\n",
    "\n",
    "plt.legend(title=src)\n",
    "\n",
    "# fig1.savefig(\"fig1.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Observation-model comparisons\n",
    "\n",
    "Here we provide examples of model-observation comparisons\n",
    "\n",
    "The simplest case is where we use a code like `settle` (see e.g. [Goodwin et al. 2019](https://ui.adsabs.harvard.edu/abs/2019MNRAS.490.2228G)) which predicts burst recurrence time and fluence, for a given accretion rate. We describe below how to do the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the 1820-30 observations on 1997 May 27\n",
    "\n",
    "print ('4U 1820-303 epoch {}'.format(obs.table[6][0]))\n",
    "\n",
    "tdel = cd.decode_LaTeX(obs.table[6][3])\n",
    "fluen, fluene  = cd.decode_LaTeX(obs.table[6][7])\n",
    "bc, bce = cd.decode_LaTeX(obs.table[6][5])\n",
    "\n",
    "d = 7.6*u.kpc\n",
    "i = 50*u.deg\n",
    "X_0 = 0.17\n",
    "Z_CNO = 0.02\n",
    "\n",
    "# We use the inferred mdot as the input value for the simulations, but in principle\n",
    "# any value can be used\n",
    "\n",
    "mdot = 0.144 # in units of mdot_Edd\n",
    "\n",
    "# this is how we would convert to mdot in g/s/cm^2 as used internally in settle\n",
    "# mdot *= 1.75*(1.7/(1+X_0))*(1e-8)*(5.01837638e24*u.g/u.s)/(11.2*u.km)**2\n",
    "# but gal17a throughout uses a fixed value of 8.8×10^4 g cm−2 s−1\n",
    "mdot *= 8.8e4*u.g/u.cm**2/u.s * 4.*np.pi*(11.2*u.km)**2\n",
    "\n",
    "# Settle model predictions give the following\n",
    "# >>> settle(0.15, 0.02, 0.17, 0.144, 1, 1.4, 11.2)\n",
    "# rec.array([(2.24151435, 3.13472479, 161.05916546)],\n",
    "#           dtype=[(‘tdel’, ‘<f8’), (‘E_b’, ‘<f8’), (‘alpha’, ‘<f8’)])\n",
    "\n",
    "fper_mod = cd.fper(mdot, (d, i, cd.OPZ, 0.0), c_bol=1.45)\n",
    "# Use lum_to_flux to convert the burst energy to fluence; note the burst flag is set\n",
    "fluen_mod = cd.lum_to_flux(3.13472479*1e39*u.erg/u.s, d, inclination=i, c_bol=1.,\n",
    "                           burst=True)*u.s\n",
    "\n",
    "# Now print out the comparison; this uses the values from a few cells up\n",
    "\n",
    "print ('\\n Parameter                         | Model | Observation  \\n-----------------------------------------------------------')\n",
    "print (' Persistent flux (1e-9 erg/cm^2/s) | {:.3f} | {:.3f}+/-{:.3f}'.format(fper_mod*1e9/(flux_unit), fper*bc, e_fper*bc))\n",
    "print (' Burst recurrence time (hr)        | {:.3f} | {:.3f}+/-{:.3f}'.format(2.24151435, tdel[0], tdel[1]))\n",
    "print (' Burst fluence (1e-6 erg/cm^2)     | {:.3f} | {:.3f}+/-{:.3f}'.format((fluen_mod*1e6/(u.erg/u.cm**2)).decompose(), fluen, fluene))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full comparison is where we also incorporate the comparison of the lightcurve. That's described here (for a different source)\n",
    "\n",
    "We use one of the \"reference\" bursts described by [Galloway et al. (2017)](http://adsabs.harvard.edu/abs/2017PASA...34...19G), provided the data is available in the `data` subdirectory of the concord repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read in some test data and generate an ObservedBurst object\n",
    "\n",
    "obs = cd.ObservedBurst.ref('GS 1826-24', 3.5)\n",
    "\n",
    "obs.plot()\n",
    "\n",
    "obs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison is a Kepler model burst lightly tuned to match this example\n",
    "\n",
    "M_NS, R_NS = 1.4*const.M_sun, 11.2*u.km\n",
    "\n",
    "opz = cd.redshift(M_NS, R_NS)\n",
    "\n",
    "c_loZ = cd.KeplerBurst(filename='mean3.data',path='example_data',\n",
    "                  lAcc=0.1164,Z=0.005,H=0.7,\n",
    "                  tdel=3.32/opz,tdel_err=0.07/opz,\n",
    "                  g = 1.858e+14*u.cm/u.s**2, R_NS=11.2*u.km) # R_Newt=10*u.km) # would be more precise\n",
    "\n",
    "print (type(c_loZ), c_loZ.mdot)\n",
    "\n",
    "# c_loZ.plot()\n",
    "\n",
    "c_loZ.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally we can use the compare method to do the comparison, which returns a likelihood\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "\n",
    "l = obs.compare(c_loZ,param = [7.5*u.kpc,60.*u.degree,1.31,-6.5*u.s], plot=True)\n",
    "plt.xlabel('Time (s)')\n",
    "\n",
    "print (\"Likelihood related to the comparison (for the parameters provided) is {:.2f}\".format(l))\n",
    "# plt.savefig(\"lc_comp.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
